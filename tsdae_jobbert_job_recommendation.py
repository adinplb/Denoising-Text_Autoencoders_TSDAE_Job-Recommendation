# -*- coding: utf-8 -*-
"""TSDAE_JOBBERT_Job Recommendation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qBD4EsLEQx8-33zQEXSDutFqXdhOFigL

# Import Necessary Library and Load Dataset
"""

pip install -U sentence-transformers

import torch
import numpy as np
from tqdm.auto import tqdm
from sentence_transformers import SentenceTransformer
from sentence_transformers.util import batch_to_device, cos_sim
from nltk import word_tokenize
from nltk.tokenize.treebank import TreebankWordDetokenizer
from tqdm.auto import tqdm
import pandas as pd
import random

>>> import nltk
>>> nltk.download('punkt_tab')

"""# Load JobBERT-v2 Model"""

# Load the model
model = SentenceTransformer("TechWolf/JobBERT-v2")

def encode_batch(jobbert_model, texts):
    features = jobbert_model.tokenize(texts)
    features = batch_to_device(features, jobbert_model.device)
    features["text_keys"] = ["anchor"]
    with torch.no_grad():
        out_features = jobbert_model.forward(features)
    return out_features["sentence_embedding"].cpu().numpy()

def encode(jobbert_model, texts, batch_size: int = 8):
    # Sort texts by length and keep track of original indices
    sorted_indices = np.argsort([len(text) for text in texts])
    sorted_texts = [texts[i] for i in sorted_indices]

    embeddings = []

    # Encode in batches
    for i in tqdm(range(0, len(sorted_texts), batch_size)):
        batch = sorted_texts[i:i+batch_size]
        embeddings.append(encode_batch(jobbert_model, batch))

    # Concatenate embeddings and reorder to original indices
    sorted_embeddings = np.concatenate(embeddings)
    original_order = np.argsort(sorted_indices)
    return sorted_embeddings[original_order]

"""# Job Corpus"""

# Example usage
job_titles = pd.read_csv("/content/drive/MyDrive/(Priority) Job Recommendation References/TF-IDF /combined_jobs_2000.csv")
job_titles

"""# Denoising Text"""

def denoise_text(text, method='a', del_ratio=0.6, word_freq_dict=None, freq_threshold=100):
    words = word_tokenize(text)
    n = len(words)
    if n == 0:
        return text

    if method == 'a':
        # === (a) Random 60% Deletion ===
        keep_or_not = np.random.rand(n) > del_ratio
        if sum(keep_or_not) == 0:
            keep_or_not[np.random.choice(n)] = True
        result = np.array(words)[keep_or_not]

    elif method == 'b':
        # === (b) Remove 60% of high-frequency words ===
        if word_freq_dict is None:
            raise ValueError("word_freq_dict is required for method 'b' or 'c'")
        high_freq_words = [i for i, w in enumerate(words) if word_freq_dict.get(w.lower(), 0) > freq_threshold]
        to_remove = set(random.sample(high_freq_words, int(del_ratio * len(high_freq_words)))) if high_freq_words else set()
        result = [w for i, w in enumerate(words) if i not in to_remove]

    elif method == 'c':
        # === (c) Based on (b) + shuffle remaining words ===
        if word_freq_dict is None:
            raise ValueError("word_freq_dict is required for method 'b' or 'c'")
        high_freq_words = [i for i, w in enumerate(words) if word_freq_dict.get(w.lower(), 0) > freq_threshold]
        to_remove = set(random.sample(high_freq_words, int(del_ratio * len(high_freq_words)))) if high_freq_words else set()
        result = [w for i, w in enumerate(words) if i not in to_remove]
        random.shuffle(result)  # simple shuffle, pair-aware shuffling can be added if needed

    else:
        raise ValueError("Unknown denoising method. Use 'a', 'b', or 'c'.")

    return TreebankWordDetokenizer().detokenize(result)

# Create noisy version of each job description
job_titles['noisy_text'] = job_titles['text'].fillna("").apply(lambda x: denoise_text(x))

# === Step 2: Encode the clean and noisy texts using pretrained JobBERT ===
clean_texts = job_titles['text'].fillna("").tolist()
noisy_texts = job_titles['noisy_text'].tolist()

"""# AutoEncoders (Embeddings)"""

# Use existing `encode` function already defined in your code
clean_embeddings = encode(model, clean_texts)
noisy_embeddings = encode(model, noisy_texts)

# === Step 3: Combine embeddings — you can average or concatenate ===
# Here we average the two embeddings to simulate TSDAE reconstruction learning
tsdae_embeddings = (clean_embeddings + noisy_embeddings) / 2.0

# === Step 4: Store the TSDAE-style embeddings ===
job_titles['jobbert_tsdae_embedding'] = tsdae_embeddings.tolist()

job_titles

# Calculate cosine similarity matrix
similarities = cos_sim(tsdae_embeddings, tsdae_embeddings)
print(similarities)

"""# Clustering TSDAE Embeddings (Job2Vec)"""

# === KMeans Clustering ===
num_clusters = 20
embedding_matrix = np.vstack(job_titles['jobbert_tsdae_embedding'].values)

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
kmeans.fit(embedding_matrix)

cluster_labels = kmeans.labels_

# === Create clustered DataFrame ===
df_clustered_jobbert = pd.DataFrame({
    'Job.ID': job_titles['Job.ID'].values if 'Job.ID' in job_titles.columns else range(len(job_titles)),
    'Title': job_titles['Title'].values if 'Title' in job_titles.columns else [None]*len(job_titles),
    'text': job_titles['text'].values,
    'cluster': cluster_labels,
    'original_index': job_titles.index
})

# === View count per cluster ===
print(df_clustered_jobbert['cluster'].value_counts())

user_corpus = pd.read_csv("/content/drive/MyDrive/(Priority) Job Recommendation References/user_applicant_jobs.csv")
user_corpus

# === Encode the job titles ===
texts_user = user_corpus["text"].fillna("").tolist()
embeddings_user = encode(model, texts_user) #model dari techwold/jobbert dan tanpa noise atau tidak melewati TSDAE

# Calculate cosine similarity matrix
similarities_user = cos_sim(embeddings_user, embeddings_user)
print(similarities_user)

# === Add embedding to the dataframe ===
user_corpus['jobbert_embedding'] = embeddings_user.tolist()

user_corpus

from sklearn.metrics.pairwise import cosine_similarity

'''
# === Step 1: Stack job embeddings ===
embedding_matrix = np.vstack(job_titles['jobbert_embedding'])
'''

# === Step 2: Compute cluster centroids ===
cluster_centroids = (
    df_clustered_jobbert
    .groupby("cluster")["original_index"]
    .apply(lambda idxs: np.mean(embedding_matrix[list(idxs)], axis=0))
    .tolist()
)
cluster_centroids = np.vstack(cluster_centroids)

def calculate_relative_threshold(similarities, percentile=75):
    """
    Calculate the relative similarity threshold based on a given percentile.
    """
    return np.percentile(similarities, percentile)

def evaluate_with_relative_threshold(similarities, top_n_indices, df_clustered_jobbert, threshold):
    """
    Evaluate relevance based on cosine similarity and a dynamic threshold.
    """
    relevant_docs = set()
    for idx, similarity in zip(top_n_indices, similarities):
        if similarity >= threshold:
            relevant_docs.add(df_clustered_jobbert.iloc[idx]["Job.ID"])  # Mark as relevant
    return relevant_docs

def get_top_n_local_search(embeddings_user, df_clustered_jobbert, embedding_matrix, top_n_list=[3, 5, 10, 20]):
    # === Step 1: Find the closest cluster center to the user embedding ===
    cluster_centers = kmeans.cluster_centers_
    cluster_similarities = cosine_similarity(embeddings_user, cluster_centers)
    best_cluster_id = np.argmax(cluster_similarities)

    # === Step 2: Filter jobs in the best cluster ===
    cluster_subset = df_clustered_jobbert[df_clustered_jobbert['cluster'] == best_cluster_id]
    cluster_indices = cluster_subset.index.to_numpy()
    cluster_embeddings = embedding_matrix[cluster_indices]

    # === Step 3: Compute similarity between user and job postings in the cluster ===
    similarities = cosine_similarity(embeddings_user, cluster_embeddings).flatten()
    top_indices_within_cluster = np.argsort(similarities)[::-1]

    # === Step 4: Calculate relative similarity threshold ===
    threshold = calculate_relative_threshold(similarities)

    # === Step 5: Extract top-N matches with relevance evaluation based on threshold ===
    results = {}
    for n in top_n_list:
        top_n_idx = top_indices_within_cluster[:n]
        selected_indices = cluster_indices[top_n_idx]
        top_n_df = df_clustered_jobbert.loc[selected_indices].copy()
        top_n_df["similarity"] = similarities[top_n_idx]

        # Evaluate relevance based on relative similarity threshold
        relevant_docs = evaluate_with_relative_threshold(similarities[top_n_idx], top_n_idx, df_clustered_jobbert, threshold)
        top_n_df["relevance_label"] = top_n_df["similarity"].apply(lambda x: "relevant" if x >= threshold else "not relevant")



        results[f"top_{n}"] = top_n_df[['Job.ID', 'Title', 'text', 'cluster', 'similarity', 'relevance_label']]

    return results

# === Step 4: Calculate relative similarity threshold ===
    threshold = calculate_relative_threshold(similarities)
    print(f"\n[Threshold] Relative Similarity Threshold (75th percentile): {threshold:.4f}")

# === Step 1: Find user query text ===
query_text = "java developer"

# Find the matching row in user_corpus
user_q_row = user_corpus[user_corpus['text'].str.lower() == query_text.lower()]

# Safety check
if user_q_row.empty:
    raise ValueError(f"Text '{query_text}' not found in user_corpus.")

# === Step 2: Extract existing embedding from user_corpus ===
embeddings_user = np.array(user_q_row.iloc[0]['jobbert_embedding']).reshape(1, -1)

'''
# Get Top-N recommendations using local search
recommendations = get_top_n_local_search(embeddings_user)

# Display results
print(f"\nQuery: '{query_text}'")
print("\nTop-3:\n", recommendations['top_3'])
print("\nTop-5:\n", recommendations['top_5'])
print("\nTop-10:\n", recommendations['top_10'])
print("\nTop-20:\n", recommendations['top_20'])
'''

recommendations = get_top_n_local_search(embeddings_user, df_clustered_jobbert, embedding_matrix)

print(f"\nQuery: '{query_text}'")
for k, df in recommendations.items():
    print(f"\n=== {k.upper()} (Threshold: {threshold:.4f}) ===")
    print(df[['Job.ID', 'Title', 'cluster', 'similarity', 'relevance_label']])

query_text

df

from sklearn.decomposition import PCA
from sklearn.metrics import pairwise_distances
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import pandas as pd
from IPython.display import display, HTML

# === Step 1: Extract top 10 recommendations ===
top_matches_10 = recommendations['top_10'].copy()
top_indices = top_matches_10.index

# === Step 2: Stack user and job embeddings (JobBERT) ===
job_embeddings_top_10 = embedding_matrix[top_indices]
combined_vectors = np.vstack([embeddings_user, job_embeddings_top_10])  # (1 + 10, dim)

# === Step 3: Reduce to 3D using PCA ===
pca = PCA(n_components=3)
reduced_embeddings = pca.fit_transform(combined_vectors)

# === Step 4: Create DataFrame for 3D plotting ===
plot_df = pd.DataFrame(reduced_embeddings, columns=['x', 'y', 'z'])
plot_df['type'] = ['QUERY'] + ['JOB'] * len(top_matches_10)
plot_df['cluster'] = ['Query'] + top_matches_10['cluster'].astype(str).tolist()
plot_df['similarity'] = [1.0] + top_matches_10['similarity'].tolist()
plot_df['text'] = ['User Query'] + top_matches_10['text'].tolist()
plot_df['title'] = ['User Query'] + top_matches_10['Title'].tolist()

# Assign a distinct color to each point (loop colors if not enough)
color_palette = px.colors.qualitative.Plotly
plot_df['color'] = [color_palette[i % len(color_palette)] for i in range(len(plot_df))]

# === Step 5: Create base scatter plot with Plotly ===
fig = px.scatter_3d(
    plot_df,
    x='x', y='y', z='z',
    color='type',
    size='similarity',
    symbol='type',
    hover_name='title',
    hover_data={'text': False, 'similarity': True, 'cluster': True},
    title='3D Visualization of Top 10 JobBERT Recommendations + User Query',
    width=800, height=700
)

fig.update_traces(marker=dict(size=7))
fig.update_layout(
    scene=dict(
        xaxis_title='PCA 1',
        yaxis_title='PCA 2',
        zaxis_title='PCA 3'
    ),
    legend_title='Type'
)

# === Step 6: Add distance lines from query to each job ===
query_coords = plot_df.iloc[0][['x', 'y', 'z']].values
job_coords = plot_df.iloc[1:][['x', 'y', 'z']].values
job_texts = plot_df.iloc[1:]['text'].tolist()

# Compute Euclidean distances in 3D space
distances = pairwise_distances([query_coords], job_coords).flatten()
plot_df.loc[1:, 'distance_from_query'] = distances

for i, (job_coord, dist, job_label) in enumerate(zip(job_coords, distances, job_texts)):
    fig.add_trace(go.Scatter3d(
        x=[query_coords[0], job_coord[0]],
        y=[query_coords[1], job_coord[1]],
        z=[query_coords[2], job_coord[2]],
        mode='lines',
        line=dict(color='gray', width=2, dash='dot'),
        hoverinfo='text',
        text=[f"Distance: {dist:.4f} to '{job_label[:30]}...'"],
        showlegend=False
    ))

fig.show()

# === Step 7: Color-coded External Label List ===
html_labels = "<h3 style='margin-top:10px;'>Legend: Matching Jobs and Query</h3><ul style='list-style:none;'>"
for i, row in plot_df.iterrows():
    dot_color = row['color']
    title = row['title']
    cluster = row['cluster']
    similarity = row['similarity']
    label_type = row['type']
    distance = row.get('distance_from_query', None)

    html_labels += f"""
    <li style="margin-bottom:8px;">
        <span style="display:inline-block;width:15px;height:15px;background:{dot_color};margin-right:10px;border-radius:50%;"></span>
        <strong>{label_type}</strong> — <em>{title}</em><br>
        <small>Cluster: {cluster} | Similarity: {similarity:.4f}"""
    if distance is not None:
        html_labels += f" | Distance: {distance:.4f}"
    html_labels += "</small></li>"

html_labels += "</ul>"

# Display HTML in Colab
display(HTML(html_labels))

from sentence_transformers.evaluation import InformationRetrievalEvaluator

# === Evaluation Prep ===

# Step 1: Define query and its ID
query_id = "q1"
queries = {query_id: query_text}  # from earlier: query_text = "java developer"

# Step 2: Define corpus (top-10 jobs) and map as ID => text
top_matches_10 = recommendations["top_10"]
corpus = {f"d{i}": row["text"] for i, (_, row) in enumerate(top_matches_10.iterrows())}

# Step 3: Define relevant documents dynamically based on similarity threshold
relevant_docs_set = set()
for i, (_, row) in enumerate(top_matches_10.iterrows()):
    if row["similarity"] >= threshold:
        relevant_docs_set.add(f"d{i}")

# Step 4: Prepare relevance dictionary
relevant_docs = {query_id: relevant_docs_set}

# Step 5: Initialize and run evaluator (model already loaded as JobBERT)
ir_evaluator = InformationRetrievalEvaluator(
    queries=queries,
    corpus=corpus,
    relevant_docs=relevant_docs,
    name="JobBERT-Top10-Eval",
    show_progress_bar=False
)

results = ir_evaluator(model)

# === Print Results in Required Format ===
print(f"\nScore-Function: cosine")
for metric, score in results.items():
    print(f"{metric}: {score * 100:.2f}%")
print(f"\nPrimary Metric: {ir_evaluator.primary_metric}")
print(f"Primary Metric Score: {results[ir_evaluator.primary_metric]:.4f}")













# === Step 1: Find user query text ===
query_text2 = "web developer"

# Find the matching row in user_corpus
user_q_row = user_corpus[user_corpus['text'].str.lower() == query_text.lower()]

# Safety check
if user_q_row.empty:
    raise ValueError(f"Text '{query_text}' not found in user_corpus.")

# === Step 2: Extract existing embedding from user_corpus ===
embeddings_user = np.array(user_q_row.iloc[0]['jobbert_embedding']).reshape(1, -1)

recommendations = get_top_n_local_search(embeddings_user, df_clustered_jobbert, embedding_matrix)

print(f"\nQuery: '{query_text2}'")
for k, df in recommendations.items():
    print(f"\n=== {k.upper()} ===")
    print(df[['Job.ID', 'Title', 'cluster', 'similarity']])

query_text2

df















"""# JobAds Corpus"""